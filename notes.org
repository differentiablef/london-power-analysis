#+TITLE: London Power Notes.

* Imports, Data, and IPython Kernel
** Initialization

Setup the dependencies for the remaining source blocks in this note.

#+name: initialize-env
#+begin_src ipython :session notes :exports both :results silent

  %matplotlib inline
  import os, sys
  import pandas as pd
  import matplotlib.pyplot as plt
  import numpy as np
  from pandas.plotting import register_matplotlib_converters
  
  register_matplotlib_converters()
  os.chdir('/home/burned/current/data-sci/london-power-analysis/')
  #plt.style.use('default')

#+end_src

and load the data currently of interest,

#+name: load-data
#+begin_src ipython :session notes :exports both :results silent

  data = pd.read_pickle('./pickle/2013-pivot.pkl')

#+end_src

** TODO Pickled Data Catalog
=TODO=
* Observations
** Distribution of kWh Readings (2013)
To get a feel for the behavior of the sample paths, it's worth looking at the distribution of the values they take.

*** Sample Population
Their full range is, 

#+BEGIN_SRC ipython :session "notes" :results silent

  values = set()
  for iid in data.columns:
      values = values | set(data[iid].fillna(-1))
      
#+END_SRC

#+RESULTS:

The concentration of readings can be roughly plotted as follows

#+BEGIN_SRC ipython :session "notes" :results silent

  plt.figure(figsize=(6,3))
  plt.hist(sorted(values), bins=200)
  plt.xlabel('Reading Value (binned)')
  plt.ylabel('Frequency')
  plt.title('Histogram: Readings')
  plt.tight_layout()
  plt.savefig('./images/concentration-values-2013.png')

#+END_SRC

#+CAPTION: "Concentration of kWh Values"
[[./images/concentration-values-2013.png]]

*** Distribution of Readings
**** The Entire Sample
Combining all the different sample paths, we can compute the distribution of readings for the entire sample. 

#+begin_src ipython :session notes :results silent :exports both

  # combine all readings into one series
  sample = pd.Series(data.values.flatten()).fillna(-1)

  # compute frequency of each reading
  freqs = sample.value_counts().sort_index()

#+end_src


Now that we've got the frequencies computed, let's check for outliers. Printing the top 5 values sorted by frequency, 

#+begin_src ipython :session notes :results silent :exports both

  # display the top-10 most frequent values.
  print(freqs.sort_values(ascending=False).head(5))

#+end_src

we get the following,

#+begin_src
 -1.000    3135940
  0.000     939581
  0.054     398987
  0.055     398709
  0.056     398284
 dtype: int64
#+end_src

Which suggests that both NaN's and zero readings might be an issue. To see if we can possibly restrict to a sub-sample, we need to look at just how frequently they occur among our sample paths,

#+name: calculate-path-scores
#+begin_src ipython :session notes :results silent :exports both

  # fill NaN with distinct value
  pathsamp = data.fillna(-1)

  # count NaN/Zero occurances per sample path.
  nan_counts = (pathsamp == -1).apply(pd.Series.value_counts).fillna(0)
  zero_counts = (pathsamp == 0).apply(pd.Series.value_counts).fillna(0) 

  # get relative frequency of NaN/Zero values for each sample path.
  freq_bad = (nan_counts.loc[True,:]+zero_counts.loc[True,:])/(data.shape[0])

  # compute cumulative distribution function of NaN/Zero count
  cdf = lambda pct : freq_bad.loc[freq_bad < pct].shape[0]/data.shape[1]

  # free up some resources
  del pathsamp

  # calculate and plot results
  prop_bad_vals = np.linspace(0,1,50)
  prop_paths = [ cdf( pct ) for pct in prop_bad_vals ]

#+end_src

After playing around for a bit, we obtain

#+begin_src ipython :session notes :results silent :exports both

  # generic plot stuff
  plt.figure(figsize=(6,4.5))
  plt.xlabel("Proportion of {NaN, 0.0} Values ($x$)"); plt.xlim(0,1)
  plt.ylabel("Proportion of Paths ($y$)"); plt.ylim(0,1)

  # select cut off proportion
  cut_off = 0.1

  # plot cdf and values
  plt.hlines( [cdf(cut_off) ], 0, 1, alpha=0.5, color='black', linestyle='-',  
              label=f'$y={cdf(cut_off):.02}$')
  plt.vlines( [cut_off], 0, 1, alpha=0.55, color='blue', 
              label=f'$x={cut_off:.02}$')

  plt.plot( prop_bad_vals, prop_paths, label='cdf', color='orange')
  plt.legend(loc='lower right'); 
  plt.tight_layout()
  plt.grid(True)

  # save plot
  plt.savefig('./images/nan-zero-values-2013.png')

#+end_src

produces, 

#+CAPTION: "Nan, Zero Value Dist."
[[./images/nan-zero-values-2013.png]]

Making it clear that excluding those paths with more than 10.0% of their values either =NaN= or =0.0=, retains approximately 90.0% of the sample. Moreover, restricting to this subset allows us to freely ignore 'NaN' values while still retaining a portion of zeros. Doing this and recalculating the frequencies for this new sample, 

#+name: extract-valid-paths
#+begin_src ipython :session notes :results silent :exports both 

  # consider only those paths which are below our threshold of 0.1
  valid = freq_bad.loc[freq_bad < 0.1].index

  # form the collection of valid paths
  paths = data.loc[:, valid]

  # and store them for later use
  pd.to_pickle(paths, './pickle/2013-10x90-samp.pkl')

  # generate un-grouped sample  
  sample = pd.Series(paths.values.flatten()).dropna()

  # compute frequency of each reading
  freqs = sample.value_counts().sort_index()/len(sample)

  # calculate mu and sigma
  mu = sample.mean(); sigma = sample.std();

#+end_src

we can then produce a plot of the observed distribution,

#+begin_src ipython :session notes :results silent :exports both
  # plot the frequencies vs. readings
  plt.figure(figsize=(12/1.5,5/1.5))
  plt.plot(freqs)
  plt.xlim(0, 3); plt.xlabel('kWh Reading');
  plt.ylim(0, 1.1*max(freqs)); plt.ylabel('Rel. Freq.')

  # show lines corresponding to  mu + k*sigma for k=0, .., 4
  plt.vlines([mu], 0, 1.1*max(freqs), label='$\mu$', color='darkblue')
  for k in range(1, 5):
      plt.vlines([mu+k * sigma], 0, 1.1*max(freqs), 
                 label=f'$\mu+{k}\sigma$', 
                 color='green', alpha=(1/k**2)*0.75, linestyle='--')

  plt.legend() # show legend
  plt.tight_layout() # clean layout
  # save plot
  plt.savefig('./images/dist-readings-2013.png')

#+end_src

obtaining,

#+CAPTION: "Distribution of kWh Readings"
[[./images/dist-readings-2013.png]]

**** Conditioned on Time and Date

To extract the distribution of values for each day/time, we do the following

#+NAME: compute-path-hist
#+begin_src ipython :session notes :results silent :exports both

  # compute frequencies of values for each time index using thresholded sample
  hist = paths.apply( lambda x : x.value_counts(), axis=1 )

  # save for later use
  pd.to_pickle( hist, './pickle/2013-10x90-hist.pkl')

#+end_src


Then, combine the results by binning the values into 350 equally sized buckets using =pd.qcut=. Since there are =6735= district values in the full population, binning the readings in this way, produces intervals in which roughly 20 values can occur.

#+NAME: compute-date-dist
#+begin_src ipython :session notes :results silent :exports both

  # tranpose so that values are the index
  hist_dt = hist.transpose(); 

  # bin values into 300 equally sized buckets
  hist_dt.index = pd.qcut( hist_dt.index, 350 )

  # combine frequencies
  hist_dt = hist_dt.groupby(hist_dt.index).sum()

  # replace intervals in the index with their midpoints
  hist_dt.index = hist_dt.index.map(lambda x: x.mid)

#+end_src

Finally to get a feel for how the distributions change with time, lets produce a density plot for a couple of days and restricted range of values. We do this as follows,

#+begin_src ipython :session notes :results silent :exports both

  # select distribution for the first two weeks if March 
  #   and only for readings less than 2.0 (2 > mu + 6 * sigma)

  # since the index is categorical, we need to find the category 
  #  corresponding to "just before 2.0"
  cpoint = max([val \
      for val in hist_dt.index.values if val < 2.0]) 

  # then extract the appropriately restricted region,
  region = hist_dt.loc[hist_dt.index < cpoint, '2013-12-15':'2013-12-30']

  days = [d for d in region.columns \
                  if d.hour == d.minute  and d.minute == 0]

  labels=[f'{day.month}-{day.day}' for day in days]

  # plot the resulting density
  plt.figure(figsize=(8, 3.5))
  plt.title('kWh Readings Density (given Date and Time)', {'fontsize': 'medium'})
  plt.pcolormesh(region.columns, region.index.values, region )
  plt.xticks(days, rotation='vertical', labels=labels)
  plt.colorbar(); 
  plt.tight_layout()
  plt.savefig('./images/dist-readings-by-time-2013.png')

#+end_src

which yields,

#+CAPTION: "Density Plot of Reading Distribution by Datetime"
[[./images/dist-readings-by-time-2013.png]]

Moreover we can clearly see the day/night cycle (as expected) as well as a change in intensity during the day, starting slightly before the 25th.

**** Conditioned on Time

Assuming we've defined 'hist_dt' as above, we'll start with it. Proceeding as before, we'll aggregate frequencies which have the same label, only in this case the label will be the time of day,

#+NAME: compute-time-dist
#+begin_src ipython :session notes :results silent :exports both
  # unit time tic is 30 minutes
  tic = pd.Timedelta(30, unit='m')

  # extract days from the sample,
  days = np.array([ d for d in hist_dt.columns \
                    if d.hour == d.minute and d.minute == 0 ])

  # compute times occurring in a day.
  times = [ tic * n for n in range(0, 48) ]

  # initialize DataFrame with appropriate index and columns
  hist_t = pd.DataFrame(index=hist_dt.index, columns=times)

  # combine frequencies based on time
  for time in times:
      hist_t.loc[:, time] = hist_dt.loc[ :, days + time].sum(axis=1)

#+end_src

Plotting the resulting density for the same slice of the sample population as before, 

#+begin_src ipython :session notes :exports both :results silent

    # plot the resulting density
    plt.figure(figsize=(6/2, 10/2))

    plt.pcolormesh(
        hist_t.columns.astype('timedelta64[m]'),
        hist_t.loc[hist_t.index < cpoint,:].index,
        hist_t.loc[hist_t.index < cpoint,:]/hist_t.sum().sum() )
  
    plt.colorbar(); 
    plt.title('kWh Density (given Time)',  {'fontsize': 'medium'})
    plt.xlabel('time (s)')
    plt.tight_layout()

    plt.savefig('./images/dist-readings-only-time-2013.png')
#+end_src

we obtain,

#+CAPTION: "Distribution of kWh Readings by Time"
[[./images/dist-readings-only-time-2013.png]]

For a different perspective, we can plot the individual curves for some set of times,

#+begin_src ipython :session notes :exports both :results silent
  # plot the resulting density  plt.figure(figsize=(6/2, 10/2))

  nhist = hist_t/hist_t.sum()

  xvals = nhist.index.values.astype('float')
  samp_times = pd.Series(times).sample(10)

  plt.plot(xvals, 
           nhist[samp_times])
  plt.xlim(0.0,2.0); plt.ylim(0, 0.16)
  plt.legend(labels=[ t.seconds//60 for t in samp_times ])
  plt.tight_layout()
  plt.savefig('./images/dist-profiles-samp-time-2013.png')
#+end_src

which results in 
#+CAPTION: "Dist. Profiles (n=10)"
[[./images/dist-profiles-samp-time-2013.png]]

*** Number of Distinct Readings
**** Per Sample Path
Looking at the number of distinct readings which occur for each sample path, we proceed by executing

#+BEGIN_SRC python
# calculate number of unique values which occur in each sample path
counts_path = data.apply(lambda x : x.nunique())

# generate histogram plot
plt.figure(figsize=(6,3))
plt.hist(counts_path, bins=70)
plt.xlabel('Num. Of Distinct Values')
plt.ylabel('Frequency')
plt.title('Dist. of Distinct Values (paths)')
plt.tight_layout()
plt.savefig('./images/dist-values-path-2013.png')
#+END_SRC 

which yields the following,

#+CAPTION: "Dist. of Reading Counts"
[[./images/dist-values-path-2013.png]]`

**** Per Time Interval
Next, the number of distinct readings per time interval,

#+BEGIN_SRC python
# calculate number of unique values which occur
counts_time = data.apply(lambda x : x.nunique(), axis=1)

# generate histogram plot
plt.figure(figsize=(6,3))
plt.hist(counts_time, bins=70)
plt.xlabel('Num. Of Distinct Values')
plt.ylabel('Frequency')
plt.title('Dist. of Distinct Values (time)')
plt.tight_layout()
plt.savefig('./images/dist-values-time-2013.png')
#+END_SRC

#+CAPTIOM: "Dist. Distinct Readings"
[[./images/dist-values-time-2013.png]]

** Total Consumption (2013)
For the year of 2013, the total power usage for a household seems strongly related to that unique household (modulo some outliers.) 

Computing the total consumption goes as follows,

#+begin_src ipython :session notes :results silent :exports both

  # load complete data for 2013.
  #   (in this file, the time-series for each household is given it's own column.)
  data = pd.read_pickle('./pickle/2013-pivot.pkl')

  # setup a place to store the results
  total = pd.DataFrame(index=data.columns)
  for iid in data.columns: 
      # compute total consumption
      total[iid] = data.loc[:,iid].fillna(0.0).sum()
    
#+end_src

(in this case, filling NaN with 0.0, is the same as dropping them.)

As an indication of how strongly connected total consumption is with the household it came from, lets look at the sample size and the number of distinct values for total consumption. In particular the following,

#+begin_src python

  sample_size = len(total.index); num_values = len(total.unique())
  print(f'Samples:           {sample_size}\n'
        f'Unique Values:     {num_values}\n'
        f'Pct. Diff:         {(sample_size - num_values)/sample_size:.02%}')

#+end_src

produces

#+begin_src
 Samples:           4411
 Unique Values:     4251
 Pct. Diff:         3.63%
#+end_src

** MAC004863 - Strong periodic signal.
#+CAPTION: "Interesting usage profile"
[[./images/MAC004863-sample.png]]

Possibly a broken appliance? Almost every night at exactly mid-night something kicks on and runs until day break.

* Useful Snippets 
** Load Data 
Current data loading procedure

To load data for an entire year, use

#+BEGIN_SRC python
  # select year
  year = 2013

  # load pivoted data (columns correspond to sample paths) 
  data = pd.read_pickle(f'./pickle/{year}-pivot.pkl')

#+END_SRC

To load only data for some months, use

#+begin_src python

  # current region of interest
  year=2013
  months=[1,2,3, 5, 8, 11, 12]

  # load months for year
  data = pd.concat([ 
      pd.read_pickle(f'./pickle/{year}-{m:02}.pkl') \
          for m in months ])

  data.reset_index(level=0, inplace=True)

  # pivot sample paths into columns
  data = data.pivot(columns='id', values='kWh')

#+end_src

** Mean Over Profile Window
The following will compute the mean 'tic' resolution profile for each sample signal (which in this case correspond to columns.)

#+begin_src python

  # samples with (almost) complete recs
  comp = data.loc[:, valid].fillna(0) # indexed by time

  # base tic and len of profile
  delta = pd.Timedelta(minutes=30)
  win_len = 48

  # our profile window
  profile = [ delta*ii  for ii in range(1,win_len) ]

  # select profile starting positions 
  starts = np.array([ dt for dt in comp.index \
                              if dt.hour == 0 and dt.minute ==0 ])

  profile_mean = pd.DataFrame(index=profile, 
         columns=comp.columns,
         dtype='float') 

  profile_std = profile_mean.copy() 

  for tic in profile:
      profile_mean.loc[tic] = comp.loc[starts+tic].mean() 
      profile_std.loc[tic] = comp.loc[starts+tic].std() 

  mean = profile_mean.mean(axis=1) 
  std = profile_mean.std(axis=1) 

#+end_src

** Transition Probabilities 

The following calculates the transition probabilities for some sample path whose associated id is 'samp_id'

#+begin_src python

  # determine which states the sample visits
  states = data[samp_id].unique()

  # initialize transition probabilities variable
  prob_trans=dict()

  # compute transition  frequencies for each state
  for state in states:
      # initialize state's entry in transition matrix
      prob_trans[state] = dict()

      # find every where the state has been observed
      observed = (data[samp_id] == state)
    
      # get the corresponding observation times
      times = data.loc[observed, samp_id].index

      # update transition frequencies
      for t in times:
          if t+dt in data[samp_id].index:
              new_state = data.loc[t+dt, samp_id]
              prob_trans[state][new_state] = \
                  prob_trans[state].setdefault(new_state, 0.0) + 1

      # normalize frequencies
      for state in prob_trans:
          sigma = sum(prob_trans[state].values())
          for new_state in prob_trans[state]:
              prob_trans[state][new_state] = prob_trans[state][new_state]/sigma

#+end_src

Moreover, using the transition distribution, the following generates sample paths and plots them

#+begin_src python

  samples=30
  paths=[]
  for ii in range(0, samples):
      # randomly select initial state
      path = [np.random.choice( trans_dist[samp_id] )]
    
      # generate path
      for dt in data[samp_id].index:
          # current path state
          state = path[-1]
        
          # select new state using trans_prob[state] distribution
          path.append( 
             np.random.choice( trans_prob[state].keys(), 
                               f=trans_prob[state].values() ))

      # add complete path to list
      paths.append(path)

#+end_src

..
